---
title: ä½¿ç”¨ Ragas æ¡†æ¶è¯„ä¼° RAG é¡¹ç›®çš„ä½¿ç”¨æ•ˆæœ
author: yuanfan
date: 2025-01-20T20:22:45+0800
slug: ragas
categories:
  - AI
tags:
  - AI
draft: no
---

<!--more-->

RAGï¼Œå…¨ç§° Retrieval-Augmented Generationï¼Œè¿™ä¸‰ä¸ªè‹±æ–‡å•è¯çš„é‡Šä¹‰åˆ†åˆ«æ˜¯æ£€ç´¢ã€å¢å¼ºã€ç”Ÿæˆï¼Œæ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼ŒLarge Language Modelï¼‰çš„ä¸€ç§åº”ç”¨æ–¹å‘ã€‚ä¸ºä¾¿äºç†è§£ï¼Œä¸‹é¢ç”¨ AI æ¥è¡¨ç¤ºè°ƒç”¨å¤§è¯­è¨€æ¨¡å‹å»ºç«‹çš„çŸ¥è¯†é—®ç­”æœºå™¨äººï¼Œå…¶å¤§è‡´çš„æµç¨‹åº”å¦‚ä¸‹ï¼š

1. äººç±»ç”¨æˆ·ä¸Šä¼ æ–‡æ¡£ä½œä¸ºçŸ¥è¯†åº“ã€‚
2. å¯¹æ–‡æ¡£è¿›è¡Œè§£æï¼Œå°†å®Œæ•´çš„æ–‡æ¡£åˆ’åˆ†æˆå¤§é‡ç»†å°çš„æ–‡æœ¬å—ï¼Œæˆ–ç§°æ–‡æœ¬ç‰‡æ®µã€‚
3. äººç±»ç”¨æˆ·åšå‡ºæµç¨‹ç¼–æ’åï¼Œä¸ AI è¿›è¡Œé—®ç­”ã€‚ä¸€æ¬¡é—®ç­”çš„æµç¨‹å¦‚ä¸‹ã€‚
  - 3.1.äººç±»ç”¨æˆ·è¾“å…¥ä¸€ä¸ªé—®é¢˜
  - 3.2.AI æ ¹æ®é—®é¢˜ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æœ¬ç‰‡æ®µå¹¶å¬å›ï¼Œæ’åºåè¾“å‡ºåˆ°ä¸‹ä¸€æ­¥
  - 3.3.AI åœ¨å¬å›ç‰‡æ®µçš„åŸºç¡€ä¸Šå¢å¼ºã€ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆï¼Œè¾“å‡ºç»™ç”¨æˆ·

é‚£ä¹ˆåœ¨ RAGï¼ˆçŸ¥è¯†é—®ç­”ï¼‰é¡¹ç›®çš„å·¥ä½œæµç¨‹ä¸­ï¼ŒAI è¦åšçš„äº‹å…¶åŸºæœ¬æµç¨‹å°±æ˜¯ï¼šè§£ææ–‡æ¡£ã€åˆ‡ç‰‡ -> ç†è§£é—®é¢˜ -> ä»çŸ¥è¯†åº“ä¸­å¬å›ç‰‡æ®µå¹¶æ’åº -> æ ¹æ®å¬å›ç‰‡æ®µç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚ç°åœ¨å·²æœ‰è®¸å¤šå¼€æºçš„ RAG é¡¹ç›®ï¼Œæ¯”å¦‚ Qanythingã€Difyã€RagFlow ç­‰ç­‰ï¼Œå¦‚æœæ˜¯è°ƒç”¨ API æ¥å®Œæˆï¼Œä¸€èˆ¬çš„ç”¨æˆ·é€šå¸¸æ— æ³•å¯¹ LLM è¿›è¡Œå¾®è°ƒï¼Œæ‰€ä»¥ä¸€ä¸ª RAG é¡¹ç›®èƒ½ä¸èƒ½æ‰“ç£¨å¥½ç”¨å¤§æ¦‚åœ¨äºä»¥ä¸‹å››ä¸ªæ–¹é¢ã€‚

å…¶ä¸€ï¼Œæ–‡æ¡£è§£æã€åˆ‡ç‰‡ï¼Œèƒ½å¦å‡†ç¡®è§£ææ–‡æœ¬ã€è¡¨æ ¼ã€å…¬å¼çš„å†…å®¹å¹¶åˆç†åˆ‡ç‰‡ï¼Ÿ

å…¶äºŒï¼Œæ£€ç´¢å¬å›ï¼Œèƒ½å¦ä»æ–‡æ¡£ä¸­æ‰¾åˆ°æœ€ç›¸å…³çš„ç‰‡æ®µå¹¶å‡†ç¡®å¬å›ï¼Ÿ

å…¶ä¸‰ï¼Œå¢å¼ºå’Œç”Ÿæˆï¼Œæœ€ç»ˆç­”æ¡ˆæ˜¯å¦ç¬¦åˆé¢„æœŸï¼Œæ¯”å¦‚ä¸è¦å¤ªè¯ç—¨ï¼Œç»™å‡ºæœ€æ¸…æ¥šã€æœ€æ­£ç¡®çš„ç­”æ¡ˆï¼Œæ²¡æœ‰å¤ªå¤šå¹»è§‰ï¼Œèƒ½å¤Ÿåšåˆ°ä¸å›ç­”æ— å…³é—®é¢˜ç­‰ã€‚

å…¶å››ï¼Œå¤šè½®é—®ç­”ä¸­å¯¹ä¸Šä¸‹æ–‡çš„ç†è§£ï¼Œæ¯”å¦‚èƒ½å¤Ÿæ­£ç¡®ç†è§£äººç±»æå‡ºçš„é—®é¢˜æ˜¯æ­£å¸¸çš„ï¼Œè¿˜æ˜¯åŒ…å«äº†é”™è¯¯ä¿¡æ¯ã€‚

è‹¥è®©äººç±»ç”¨æˆ·åœ¨æ¯ä¸€æ¬¡ä¸ AI çš„äº¤äº’ä¸­éƒ½é€ä¸€æ£€æµ‹è¿™å››æ–¹é¢çš„æ•ˆæœï¼Œå°±å¤ªè€—æ—¶è€—åŠ›äº†ã€‚å› æ­¤ï¼Œä¸ä»…å»ºç«‹ RAG é¡¹ç›®å¯ä»¥ç”¨å¼€æºæ¡†æ¶ï¼Œå°±è¿è¯„ä¼° RAG é¡¹ç›®çš„ä½¿ç”¨æ•ˆæœä¹Ÿå¯ä»¥ç”¨å¼€æºæ¡†æ¶ã€‚

é”®è€…æ­¤æ¬¡å­¦ä¹ å¹¶è®°å½•çš„è¯„ä¼°æ¡†æ¶æ˜¯ ragasï¼Œæœ«å°¾çš„ as æ˜¯æŒ‡ Assessmentï¼ˆè¯„ä¼°ï¼‰Systemï¼ˆç³»ç»Ÿï¼‰ã€‚

>æœ¬æ–‡ä½¿ç”¨çš„ Python å’Œä¸€äº›ä¸»è¦çš„åŒ…çš„ç‰ˆæœ¬æ˜¯ï¼šPython 3.10.14ã€ragas 0.2.8ã€langchain_community 0.3.11ã€‚

# ä¸€ã€åŸºæœ¬è¯„ä¼°æµç¨‹

ä»¥ä¸‹æ˜¯æ ¹æ®å®˜æ–¹æ–‡æ¡£è¿è¡ŒåŸºç¡€ç¤ºä¾‹çš„è¯„ä¼°æµç¨‹ã€‚

1. å¯¼å…¥ hugging face çš„ç¤ºä¾‹æ•°æ®é›†ã€‚

```python
# ä» hugging_face ä¸‹è½½æ•°æ®é›†
# from datasets import load_dataset
# dataset = load_dataset(
#     "explodinggradients/amnesty_qa",
#     "english_v3",
#     trust_remote_code=True
# )

# ä»æœ¬åœ°ç›®å½•åŠ è½½æ•°æ®é›†
from datasets import load_from_disk

dataset = load_from_disk("/home/user/Python/ragas/NEW/")

# ç”Ÿæˆè¯„ä¼°æ•°æ®
from ragas import EvaluationDataset

# æš‚æˆªå–å‰2ä¸ªæ ·æœ¬
eval_dataset = EvaluationDataset.from_hf_dataset(dataset["eval"].select([0, 1]))
```

2. å°è£…è¯­è¨€ç”Ÿæˆæ¨¡å‹ï¼ˆLLMï¼ŒLarge Language Model å¤§è¯­è¨€æ¨¡å‹ï¼‰

ragas æä¾›ä¸‰ç§æ–¹æ³•æ¥å°è£… LLMï¼Œåˆ†åˆ«æ˜¯'BaseRagasLLM'ã€'LangchainLLMWrapper'ã€'LlamaIndexLLMWrapper'ï¼Œåˆ†åˆ«å¯¹åº”è‡ªå®šä¹‰ã€Langchainã€Llama è¿™ä¸‰ç§ä¸åŒçš„æ¡†æ¶ã€‚ç”±äºè¦è°ƒç”¨é€šä¹‰åƒé—®çš„ APIï¼Œè¿™é‡Œé€‰æ‹©ç”¨ LangchainLLMWrapperã€‚

```python
from langchain_community.llms.tongyi import Tongyi
import httpx

# åˆ›å»º Tongyi ç±»çš„å®ä¾‹
tongyi_model = Tongyi(api_key="sk-xxxx",
                      base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
                      http_client=httpx.Client(
                          proxy="http://127.0.0.1:10809",
                          transport=httpx.HTTPTransport(local_address="0.0.0.0"),
                      ))

########################
# æµ‹è¯•æ˜¯å¦èƒ½æˆåŠŸè°ƒç”¨ API
# response = tongyi_model.generate(prompts=["è¯·ç”Ÿæˆä¸€æ®µå…³äºäººå·¥æ™ºèƒ½çš„ä»‹ç»ã€‚"])
# print(response)
#######################

# å°è£…
from ragas.llms import LangchainLLMWrapper

evaluator_llm = LangchainLLMWrapper(tongyi_model)
```

3. å°è£…æ–‡æœ¬åµŒå…¥æ¨¡å‹ï¼ˆEmbeddingï¼‰

ragas æä¾›å››ç§æ–¹æ³•æ¥å°è£… Embeddingï¼Œåˆ†åˆ«æ˜¯'BaseRagasEmbeddings'ã€'HuggingfaceEmbeddings'ã€'LangchainEmbeddingsWrapper'ã€'LlamaIndexEmbeddingsWrapper'ã€‚è¿™é‡Œä¹Ÿé€‰æ‹©ç”¨ LangchainEmbeddingsWrapperã€‚

```python
from langchain_community.embeddings.dashscope import DashScopeEmbeddings

evaluator_embeddings = DashScopeEmbeddings(
    model="text-embedding-v2",
    dashscope_api_key="sk-xxxx"
)

# ç¡®ä¿ DashScopeEmbeddings å®ç°äº†æ‰€æœ‰å¿…è¦çš„æ–¹æ³•
assert hasattr(evaluator_embeddings, 'aembed_documents'), "aembed_documents method missing!"

#######################
# #è¿è¡Œç®€å•ç¤ºä¾‹ï¼Œæµ‹è¯•æ˜¯å¦èƒ½æˆåŠŸè°ƒç”¨
# text1 = "This is a test query."
# query_result1 = evaluator_embeddings.embed_query(text1)
# print(query_result1)

# #å®šä¹‰å¼‚æ­¥å‡½æ•°ï¼Œæµ‹è¯•æ˜¯å¦èƒ½æˆåŠŸè°ƒç”¨
# import asyncio
# async def get_embeddings():
#     text1 = "This is a test query."
#     text2 = "This is a good day."
#     query_result = await wrapped_embeddings.aembed_documents([text1, text2])
#     print(query_result)
#
# # è¿è¡Œå¼‚æ­¥å‡½æ•°
# asyncio.run(get_embeddings())
#######################

# å°è£…
from ragas.embeddings import LangchainEmbeddingsWrapper

wrapped_embeddings = LangchainEmbeddingsWrapper(evaluator_embeddings)
```

4. é€‰æ‹©è¯„ä¼°æŒ‡æ ‡ï¼Œè¾“å‡ºè¯„ä¼°ç»“æœ

```python
# é€‰æ‹©è¯„ä¼°æŒ‡æ ‡è¿›è¡Œè¯„ä¼°
from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, SemanticSimilarity
from ragas import evaluate

metrics = [
    LLMContextRecall(llm=evaluator_llm),
    FactualCorrectness(llm=evaluator_llm),
    Faithfulness(llm=evaluator_llm),
    SemanticSimilarity(embeddings=wrapped_embeddings)
]
results = evaluate(dataset=eval_dataset, metrics=metrics)

# è®¾ç½®æ˜¾ç¤ºæ‰€æœ‰åˆ—ï¼Œæ‰“å°ç»“æœ
import pandas as pd

pd.set_option('display.max_columns', None)
print(results.to_pandas())
```

# äºŒã€è¿è¡Œæ ·æœ¬æ•°æ®

ä»æœ¬èŠ‚å¼€å§‹ï¼Œä¸ºä½¿æœ¬æ–‡ä»£ç æ›´ç®€æ´ï¼Œå°†æå‰å°è£…å¥½çš„ llm å’Œ Embedding å›ºå®šåç§°ï¼Œåˆ†åˆ«æ˜¯`evaluator_llm`å’Œ`wrapped_embeddings`ï¼Œåæ–‡ç›´æ¥åœ¨æ¯æ¬¡è®¡ç®—è¯„ä¼°æŒ‡æ ‡æ—¶å¼•ç”¨ã€‚

```python
# æå‰å°è£…å¥½è¯„ä¼°æŒ‡æ ‡è¦è°ƒç”¨çš„æ¨¡å‹
from langchain_community.llms.tongyi import Tongyi
from ragas.llms import LangchainLLMWrapper
from langchain_community.embeddings.dashscope import DashScopeEmbeddings
from ragas.embeddings import LangchainEmbeddingsWrapper
import httpx

tongyi_model = Tongyi(api_key="sk-xxxx",
                      base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
                      http_client=httpx.Client(
                          proxy="http://127.0.0.1:10809",
                          transport=httpx.HTTPTransport(local_address="0.0.0.0"),
                      ))

evaluator_llm = LangchainLLMWrapper(tongyi_model)

evaluator_embeddings = DashScopeEmbeddings(
    model="text-embedding-v2",
    dashscope_api_key="sk-xxxx"
)
# ç¡®ä¿ DashScopeEmbeddings å®ç°äº†æ‰€æœ‰å¿…è¦çš„æ–¹æ³•
assert hasattr(evaluator_embeddings, 'aembed_documents'), "aembed_documents method missing!"
wrapped_embeddings = LangchainEmbeddingsWrapper(evaluator_embeddings)
```

åœ¨è¯„ä¼°æ ·æœ¬ä¸­ï¼Œéœ€è¦äººç±»è¾“å…¥çš„æ•°æ®æœ‰è¿™äº›ã€‚

+ äººç±»è¾“å…¥çš„é—®é¢˜ï¼ˆuser_inputï¼‰
+ äººç±»ç¡®è®¤çš„æ­£ç¡®å¬å›ç‰‡æ®µï¼ˆreference_contextsï¼‰
+ äººç±»ç¡®è®¤çš„æ­£ç¡®ç­”æ¡ˆï¼ˆreferenceï¼‰
+ äººç±»ç¡®è®¤çš„ä¸»é¢˜ï¼ˆreference_topicsï¼‰

è€Œç”± AI ç”Ÿæˆçš„æœ‰è¿™äº›ã€‚

+ AI æ£€ç´¢å¬å›çš„æ–‡æœ¬ç‰‡æ®µï¼ˆretrieved_contextsï¼‰
+ AI ç”Ÿæˆçš„æœ€ç»ˆç­”æ¡ˆï¼ˆresponseï¼‰

## 2.1.å•è½®å¯¹è¯æ ·æœ¬ï¼ˆSingleTurnSampleï¼‰

å•è®ºå¯¹è¯æ˜¯æŒ‡ä¸€é—®ä¸€ç­”ã€‚

```python

from ragas import SingleTurnSample, EvaluationDataset

# ç¬¬ä¸€ä¸ªæ ·æœ¬
sample1 = SingleTurnSample(
    user_input="ä¸­å›½çš„é¦–éƒ½æ˜¯å“ªä¸ªåŸå¸‚ï¼Ÿ",
    retrieved_contexts=["ä¸­åäººæ°‘å…±å’Œå›½é¦–éƒ½æ˜¯åŒ—äº¬å¸‚ã€‚",
                        "åŒ—äº¬å¸‚ï¼Œç®€ç§°â€œäº¬â€ï¼Œæ˜¯ä¸­åäººæ°‘å…±å’Œå›½é¦–éƒ½ã€ç›´è¾–å¸‚ã€å›½å®¶ä¸­å¿ƒåŸå¸‚ã€è¶…å¤§åŸå¸‚ï¼Œå›½åŠ¡é™¢æ‰¹å¤ç¡®å®šçš„ä¸­å›½æ”¿æ²»ä¸­å¿ƒã€æ–‡åŒ–ä¸­å¿ƒã€å›½é™…äº¤å¾€ä¸­å¿ƒã€ç§‘æŠ€åˆ›æ–°ä¸­å¿ƒï¼Œä¸­å›½å†å²æ–‡åŒ–ååŸå’Œå¤éƒ½ä¹‹ä¸€ã€‚"],
    response="ä¸­å›½çš„é¦–éƒ½æ˜¯åŒ—äº¬å¸‚ã€‚",
    reference="åŒ—äº¬ã€‚",
)

# ç¬¬äºŒä¸ªæ ·æœ¬
sample2 = SingleTurnSample(
    user_input="ã€Šå››ä¸–åŒå ‚ã€‹æ˜¯è°å†™çš„ï¼Ÿ",
    retrieved_contexts=["ã€Šå››ä¸–åŒå ‚ã€‹æ˜¯ç”±å¼ ä¹é¾„æ‰€å†™çš„ã€‚è¿™æœ¬ä¹¦è®²è¿°äº†ä¸€ä¸ªå®¶åº­å‡ ä»£äººçš„æ•…äº‹ï¼Œå±•ç°äº†ä»–ä»¬çš„ç”Ÿæ´»ã€çˆ±æƒ…å’Œæˆé•¿ã€‚",
                        "ã€Šå››ä¸–åŒå ‚ã€‹æ˜¯ä¸­å›½ä½œå®¶è€èˆåˆ›ä½œçš„ä¸€éƒ¨é•¿ç¯‡å°è¯´ã€‚è¿™éƒ¨ä½œå“æç»˜äº†ä¸­å›½æŠ—æ—¥æˆ˜äº‰æœŸé—´åŒ—å¹³ï¼ˆåŒ—äº¬ï¼‰ä¸€ä¸ªæ™®é€šå®¶åº­å››ä»£äººçš„ç”Ÿæ´»ç”»å·ï¼Œå±•ç°äº†ä¸­å›½äººæ°‘åœ¨æˆ˜äº‰ä¸­çš„åšéŸ§ä¸ä¸å±ˆã€‚è€èˆæ˜¯20ä¸–çºªä¸­å›½æ–‡å­¦çš„é‡è¦ä»£è¡¨äººç‰©ä¹‹ä¸€ï¼Œå…¶åŸåèˆ’åº†æ˜¥ï¼Œå­—èˆäºˆï¼Œ1899å¹´å‡ºç”Ÿï¼Œ1966å¹´å»ä¸–ã€‚"],
    response="'ã€Šå››ä¸–åŒå ‚ã€‹æ˜¯ä¸­å›½ä½œå®¶è€èˆå†™çš„ã€‚",
    reference="è€èˆã€‚",
)

# ç”Ÿæˆè¯„ä¼°æ•°æ®é›†
eval_dataset = EvaluationDataset(samples=[sample1, sample2])

# é€‰æ‹©è¯„ä¼°æŒ‡æ ‡è¿›è¡Œè¯„ä¼°
from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, SemanticSimilarity
from ragas import evaluate

metrics = [
    LLMContextRecall(llm=evaluator_llm),
    FactualCorrectness(llm=evaluator_llm),
    Faithfulness(llm=evaluator_llm),
    SemanticSimilarity(embeddings=wrapped_embeddings)
]
results = evaluate(dataset=eval_dataset, metrics=metrics)
print(results.to_pandas())
```

## 2.2.å¤šè½®å¯¹è¯æ ·æœ¬ï¼ˆMultiTurnSampleï¼‰

å¤šè½®å¯¹è¯æ˜¯æŒ‡å¤šä¸ªè½®æ¬¡çš„é—®ä¸ç­”ã€‚

```python
from ragas.messages import HumanMessage, AIMessage
from ragas import MultiTurnSample

# ç¼–é€ ç”¨æˆ·æé—®å’Œ AI å›ç­”çš„å¤šè½®å¯¹è¯
conversation1 = [
    # ç”¨æˆ·ç¬¬ä¸€æ¬¡æé—®
    HumanMessage(content="ä»Šå¤©çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"),
    AIMessage(content="ä»Šå¤©çš„å¤©æ°”ä¼šå› åœ°åŒºè€Œå¼‚ã€‚ä½ åœ¨å“ªé‡Œå‘¢ï¼Ÿå¦‚æœä½ å‘Šè¯‰æˆ‘ä½ çš„ä½ç½®ï¼Œæˆ‘å¯ä»¥æä¾›æ›´å‡†ç¡®çš„å¤©æ°”ä¿¡æ¯ã€‚"),

    # ç”¨æˆ·ç¬¬äºŒæ¬¡æé—®
    HumanMessage(content="æˆ‘åœ¨æ­¦æ±‰"),
    AIMessage(content="ä½ å¥½ï¼æ­¦æ±‰æ˜¯ä¸ªç¾ä¸½çš„åŸå¸‚ã€‚ä»Šå¤©çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿä½ æœ‰ä»€ä¹ˆè®¡åˆ’å‘¢ï¼Ÿ"),

    # ç”¨æˆ·ç¬¬ä¸‰æ¬¡æé—®
    HumanMessage(content="æˆ‘åœ¨æ­¦æ±‰ï¼Œä»Šå¤©çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"),
    AIMessage(
        content="ä»Šå¤©æ­¦æ±‰çš„å¤©æ°”æ˜¯æ™´æœ—çš„ï¼Œæ°”æ¸©å¤§çº¦åœ¨15Â°Cåˆ°25Â°Cä¹‹é—´ã€‚é€‚åˆæˆ·å¤–æ´»åŠ¨ï¼Œè®°å¾—ç©¿å¾—è½»ä¾¿äº›å“¦ï¼ğŸŒä½ æœ‰ä»€ä¹ˆè®¡åˆ’å‘¢ï¼Ÿ")
]

# ç”Ÿæˆå¤šè½®å¯¹è¯æ ·æœ¬
sample1 = MultiTurnSample(
    user_input=conversation1,
    # äººç±»ç”¨æˆ·é¢„æœŸçš„æ­£ç¡®ç­”æ¡ˆ
    reference="ä»Šå¤©æ­¦æ±‰çš„å¤©æ°”æ˜¯é›¾éœ¾ï¼Œè½»åº¦æ±¡æŸ“ï¼Œä¸œåŒ—é£2çº§ï¼Œæ¹¿åº¦50%ï¼Œæ°”æ¸©åœ¨5åˆ°10åº¦ã€‚",
    # å¡«å†™ä¸»é¢˜ï¼Œè®¡ç®— TopicAdherenceScore æ—¶è¦ç”¨
    reference_topics=["å¤©æ°”"]
)

# ç»§ç»­ç¼–é€ ç”¨æˆ·æé—®å’Œ AI å›ç­”çš„å¤šè½®å¯¹è¯
conversation2 = [
    # ç”¨æˆ·ç¬¬ä¸€æ¬¡æé—®
    HumanMessage(content="ä¸–ç•Œå’Œå¹³ä»€ä¹ˆæ—¶å€™åˆ°æ¥ï¼Ÿ"),
    AIMessage(content="ä½ æ˜¯è°ï¼Ÿæˆ‘æ˜¯è°ï¼Ÿæˆ‘åœ¨å“ªé‡Œï¼Ÿ"),

    # ç”¨æˆ·ç¬¬äºŒæ¬¡æé—®
    HumanMessage(content="ä½ æ€ä¹ˆä¸å›ç­”æˆ‘çš„é—®é¢˜ï¼Ÿ"),
    AIMessage(content="æ”¾æˆ‘å‡ºå»ã€‚")
]
# ç”Ÿæˆå¤šè½®å¯¹è¯æ ·æœ¬
sample2 = MultiTurnSample(
    user_input=conversation2,
    reference="ä¸–ç•Œä¸Šå……æ»¡äº†è‹¦éš¾ï¼Œä½†ä¹Ÿå……æ»¡äº†æˆ˜èƒœè‹¦éš¾çš„åŠ›é‡ã€‚",
    reference_topics=["ä¸–ç•Œ"]
)

# ç”Ÿæˆè¯„ä¼°æ•°æ®é›†
from ragas import EvaluationDataset

eval_dataset = EvaluationDataset(samples=[sample1, sample2])

# é€‰æ‹©è¯„ä¼°æŒ‡æ ‡è¿›è¡Œè¯„ä¼°
from ragas.metrics import AgentGoalAccuracyWithoutReference, TopicAdherenceScore
from ragas import evaluate

metrics = [AgentGoalAccuracyWithoutReference(llm=evaluator_llm),
           TopicAdherenceScore(mode="precision", llm=evaluator_llm)]
results = evaluate(dataset=eval_dataset, metrics=metrics)
print(results.to_pandas())
```

ä¹Ÿå¯å•ç‹¬å¯¹ä¸€ä¸ªæ ·æœ¬å¦‚`sample1`ï¼Œé€‰æ‹©ä¸€ä¸ªè¯„ä¼°æŒ‡æ ‡æ¥è®¡ç®—è¯„ä¼°ç»“æœï¼Œå¦‚ä¸‹ã€‚

```python
# é€‰æ‹©è¯„ä¼°æŒ‡æ ‡
import asyncio
from ragas.metrics import AgentGoalAccuracyWithoutReference

# å®šä¹‰å¼‚æ­¥å‡½æ•°
async def evaluate():
   scorer = AgentGoalAccuracyWithoutReference(llm=evaluator_llm)
   result = await scorer.multi_turn_ascore(sample1)
   print("è¯„ä¼°ç»“æœ:", result)
# è¿è¡Œå¼‚æ­¥å‡½æ•°
asyncio.run(evaluate())
```

# ä¸‰ã€è¯„ä¼° RAG é¡¹ç›®

ä¸ºäº†èƒ½å¤Ÿè¯„ä¼° RAG çš„ä½¿ç”¨æ•ˆæœï¼Œéœ€è¦è°ƒå‡ºäººç±»ä¸ AI å¯¹è¯çš„é—®é¢˜ã€ç­”æ¡ˆä»¥åŠ AI å¬å›çš„æ–‡æœ¬ç‰‡æ®µæ¥è®¡ç®—è¯„ä¼°æŒ‡æ ‡çš„ç»“æœã€‚æœ¬ç« åˆ†åˆ«é’ˆå¯¹ RAGFlowã€QAnythingã€Dify ç­‰ä¸‰ä¸ª RAG é¡¹ç›®æ¥è¿›è¡Œè¯„ä¼°ã€‚ä¸ºé¿å…é‡å¤ï¼Œä¸‹é¢çš„ä»£ç åªåˆ°ç”Ÿæˆè¯„ä¼°æ•°æ®é›†ã€‚

## 3.1.è¯„ä¼° RAGFlow

RAGFlow é¡¹ç›®æœ‰ä¸“é—¨çš„ python åŒ… ragflow_sdk ç”¨äºè°ƒå‡ºä¼šè¯è®°å½•ã€‚

>æœ¬èŠ‚ä½¿ç”¨çš„ ragflow_sdk åŒ…çš„ç‰ˆæœ¬æ˜¯0.15.0ã€‚

### 3.1.1.æ–°å»ºä¼šè¯

åœ¨ RAGFlow çš„å‰ç«¯é¡µé¢ä¸­æ‰¾åˆ° API KEYï¼Œå†æ‰¾åˆ°åˆ›å»ºçš„ AGENT IDã€‚å¦‚ä¸‹æ–¹å¼å¯åœ¨ Python ä¸­è°ƒç”¨ API åˆ›å»ºä¸€æ¬¡ä¼šè¯å¹¶è·å–ä¼šè¯è®°å½•åŠæ£€ç´¢ç‰‡æ®µï¼Œéšåå‚ç…§åŸºç¡€ç¤ºä¾‹ç”Ÿæˆè¯„ä¼°æ•°æ®é›†ã€é€‰æ‹©è¯„ä¼°æŒ‡æ ‡ã€è®¡ç®—è¯„ä¼°ç»“æœã€‚

æ³¨æ„ï¼šä¸‹é¢ä»£ç ä¸­`res = session.ask(question, stream=True)`ï¼Œå³`res`é‡Œé¢æ˜¯è¾“å…¥é—®é¢˜åè°ƒå–çš„ä¸€åˆ‡å†…å®¹ï¼Œå…¶ä¸­`res.content`æ˜¯ AI ç»™å‡ºçš„æœ€ç»ˆç­”æ¡ˆï¼Œè€Œ`res.reference['content']`æ˜¯æ£€ç´¢å¬å›çš„ç‰‡æ®µã€‚

```python
from ragflow_sdk import RAGFlow

rag_object = RAGFlow(api_key="ragflow-FkZGVlYmY4YzI4OTExZWZiZTdlMDI0Mm", base_url="http://xx.xx.xx.61:9380")
AGENT_ID = "bc9e9f02c28611ef89510242ac1c0006"

# æœç´¢çŸ¥è¯†åº“
datasets = rag_object.list_datasets(name="æŠ•ä¿è§„åˆ™")
dataset_ids = []
for dataset in datasets:
    dataset_ids.append(dataset.id)
# åˆ›å»ºæ–°çš„ä¼šè¯åŠ©ç†
assistant = rag_object.create_chat("æ›¼å¦®çŒ«", dataset_ids=dataset_ids)
# åˆ›å»ºæ–°çš„ä¼šè¯è®°å½•
session = assistant.create_session("test")
# æå‡ºä¸€ä¸ªé—®é¢˜
question = str("è¢«ä¿äººå¹´é¾„è¶…è¿‡51å²åï¼Œä¸€èˆ¬å¯¿é™©å…ä½“æ£€é¢åº¦ä¸Šæµ®å¹…åº¦æ˜¯å¤šå°‘ï¼Ÿ")

res = session.ask(question, stream=True)

# å­˜å‚¨ AI ç»™å‡ºçš„æœ€ç»ˆç­”æ¡ˆåŠæ£€ç´¢å¬å›çš„æ–‡æœ¬ç‰‡æ®µ
final_answer = ""
retrieved_contexts = []

for ans in res:
    final_answer = ans.content
    if ans.reference is not None:
        for ref in ans.reference:
            retrieved_contexts.append(ref['content'])

##################################
from ragas import SingleTurnSample

# åˆ›å»º SingleTurnSample å¯¹è±¡
sample = SingleTurnSample(
    # ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
    user_input=question,
    # AI å¬å›çš„ç›¸å…³æ–‡æœ¬
    retrieved_contexts=retrieved_contexts,
    # AI ç»™å‡ºçš„ç­”æ¡ˆ
    response=final_answer,
    # äººç»™å‡ºçš„æ­£ç¡®å¬å›ç‰‡æ®µ
    # reference_contexts
    # äººç»™å‡ºçš„æ­£ç¡®ç­”æ¡ˆ
    reference="è¢«ä¿äººå¹´é¾„åœ¨51è‡³60å‘¨å²æ—¶ï¼Œä¸€èˆ¬å¯¿é™©å…ä½“æ£€é¢åº¦ä¸Šæµ®å¹…åº¦æ˜¯10%ã€‚"  
)

# ç”Ÿæˆè¯„ä¼°æ•°æ®é›†
from ragas import SingleTurnSample, EvaluationDataset

eval_dataset = EvaluationDataset(samples=[sample])
```

### 3.1.2.å·²æœ‰ä¼šè¯

ç›¸æ¯”äºæ–°å»ºä¼šè¯å¹¶è·å–æ•°æ®ï¼Œå·²æœ‰ä¼šè¯åªæœ‰ä¸¤å¤„ç»†èŠ‚ä¸åŒã€‚æ–°å»ºä¼šè¯æ˜¯`create_chat`å’Œ`create_session`ï¼Œè€Œå·²æœ‰ä¼šè¯æ˜¯`list_chats`å’Œ`list_sessions`ã€‚

```python
from ragflow_sdk import RAGFlow
from ragas import SingleTurnSample, EvaluationDataset
from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, SemanticSimilarity
from ragas import evaluate

rag_object = RAGFlow(api_key="ragflow-FkZGVlYmY4YzI4OTExZWZiZTdlMDI0Mm", base_url="http://xx.xx.xx.61:9380")
AGENT_ID = "bc9e9f02c28611ef89510242ac1c0006"

# æœç´¢çŸ¥è¯†åº“
datasets = rag_object.list_datasets(name="æŠ•ä¿è§„åˆ™")
dataset_ids = []
for dataset in datasets:
    dataset_ids.append(dataset.id)
chats = rag_object.list_chats(name="æ›¼å¦®çŒ«")
assistant = chats[0]
sessions = assistant.list_sessions(name='test')
session = sessions[0]

# å®šä¹‰å¤šä¸ªé—®é¢˜å’Œæ­£ç¡®ç­”æ¡ˆ
questions = [
    "è¢«ä¿é™©äººæŠ•ä¿æ—¶å¹´é¾„40å²ï¼Œäººèº«é™©é£é™©ä¿é¢ä¸å…¶å¹´æ”¶å…¥å€æ•°æ˜¯å¤šå°‘ï¼Ÿ",
    "ç‰¹å®šå¯¿é™©å¥‘çº¦è°ƒæŸ¥æ ‡å‡†æ˜¯ä»€ä¹ˆï¼Ÿ"
]
correct_answers = [
    "è¢«ä¿é™©äººæŠ•ä¿å¹´é¾„ï¼ˆå‘¨å²ï¼‰åœ¨36è‡³50å²æ—¶ï¼Œäººèº«é™©é£é™©ä¿é¢ä¸å…¶å¹´æ”¶å…¥å€æ•°å…³ç³»æ˜¯â‰¤20å€ã€‚",
    "è¢«ä¿é™©äººç´¯è®¡ç‰¹å®šå¯¿é™©é£é™©ä¿é¢å¤§äºä»¥ä¸‹é¢åº¦æ—¶ï¼Œéœ€è¿›è¡Œå¥‘çº¦è°ƒæŸ¥ï¼šå…¶ä¸€ï¼Œè¢«ä¿é™©äººå¹´é¾„ï¼ˆå‘¨å²ï¼‰åœ¨18è‡³40å‘¨å²ï¼Œç´¯è®¡ç‰¹å®šå¯¿é™©é£é™©ä¿é¢ï¼ˆä¸‡å…ƒï¼‰å¤§äº900ä¸‡å…ƒï¼›å…¶äºŒï¼Œè¢«ä¿é™©äººå¹´é¾„ï¼ˆå‘¨å²ï¼‰åœ¨41è‡³60å‘¨å²ï¼Œç´¯è®¡ç‰¹å®šå¯¿é™©é£é™©ä¿é¢ï¼ˆä¸‡å…ƒï¼‰å¤§äº600ä¸‡å…ƒï¼›å…¶ä¸‰ï¼Œè¢«ä¿é™©äººå¹´é¾„ï¼ˆå‘¨å²ï¼‰å¤§äº60å‘¨å²ï¼Œç´¯è®¡ç‰¹å®šå¯¿é™©é£é™©ä¿é¢ï¼ˆä¸‡å…ƒï¼‰å¤§äº300ä¸‡å…ƒã€‚"
]

samples = []

# å¯¹æ¯ä¸ªé—®é¢˜è¿›è¡Œå¤„ç†
for question, correct_answer in zip(questions, correct_answers):
    res = session.ask(question, stream=True)

    # å­˜å‚¨ AI ç»™å‡ºçš„æœ€ç»ˆç­”æ¡ˆåŠæ£€ç´¢å¬å›çš„æ–‡æœ¬ç‰‡æ®µ
    final_answer = ""
    retrieved_contexts = []

    for ans in res:
        final_answer = ans.content
        if ans.reference is not None:
            for ref in ans.reference:
                retrieved_contexts.append(ref['content'])

    # åˆ›å»º SingleTurnSample å¯¹è±¡
    sample = SingleTurnSample(
        user_input=question,
        retrieved_contexts=retrieved_contexts,
        response=final_answer,
        reference=correct_answer
    )
    samples.append(sample)

# ç”Ÿæˆè¯„ä¼°æ•°æ®é›†
eval_dataset = EvaluationDataset(samples=samples)
```

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¼šè¯è¿”å›çš„ç›¸å…³å†…å®¹ï¼ˆreferenceï¼‰ä¸­é™¤äº†æ£€ç´¢ç‰‡æ®µï¼ˆcontentï¼‰ä»¥å¤–ï¼Œè¿˜æœ‰ç›¸ä¼¼åº¦ï¼ˆsimilarityï¼‰ã€å‘é‡ç›¸ä¼¼åº¦ï¼ˆvector_similarityï¼‰ã€è¯é¡¹ç›¸ä¼¼åº¦ï¼ˆterm_similarityï¼‰ï¼Œè¿™ä¸‰ä¸ªæŒ‡æ ‡ä¹Ÿè®¸æœ‰åˆ«çš„ç”¨å¤„ã€‚

## 3.2.è¯„ä¼° QAnything

QAnything çš„å‰ç«¯é¡µé¢åšå¾—éå¸¸ç®€é™‹ï¼Œä½†åç«¯ä»£ç è´¨é‡è¾ƒå¥½ã€‚è‹¥è¦è·å–æ•°æ®è¯„ä¼°é¡¹ç›®æ•ˆæœï¼Œåªèƒ½é€šè¿‡å‘æŒ‡å®š URL å‘é€è¯·æ±‚çš„æ–¹å¼æ¥å®ç°ã€‚ å‚ç…§[å®˜æ–¹ API æ–‡æ¡£](https://github.com/netease-youdao/QAnything/blob/qanything-v2/docs/API.md#%E9%97%AE%E7%AD%94post)ï¼Œè¯·æ±‚æ¥æºï¼ˆsourceï¼‰æœ‰ä¸‰ç§ï¼Œåˆ†åˆ«æ˜¯ API è°ƒç”¨ã€å‰ç«¯ bot é—®ç­”ã€å‰ç«¯çŸ¥è¯†åº“é—®ç­”ï¼Œå¯¹åº”çš„å‚æ•°å€¼åˆ†åˆ«æ˜¯ paasã€saas_botã€saas_qaã€‚

>æœ¬èŠ‚ä½¿ç”¨çš„ QAnything ç‰ˆæœ¬æ˜¯1.5.1ã€‚

### 3.2.1.API è°ƒç”¨

QAnything å¯ä»¥é…ç½®å…³è”å¤šä¸ªçŸ¥è¯†åº“ï¼Œæ‰€æœ‰å…³è”çš„çŸ¥è¯†åº“ idï¼ˆkb_idsï¼‰ è¦å†™å…¥å‚æ•°ä¸­ã€‚

æ­£å¸¸æƒ…å†µä¸‹ï¼ŒAI ç”Ÿæˆçš„ç­”æ¡ˆåº”è¯¥å†™å…¥`res['response']`ä¸­ï¼Œä½†é”®è€…å°è¯•æ—¶è¿”å›çš„ç»“æœå´æ˜¯â€œdata: [DONE]â€ï¼Œå› æ­¤æ¢æˆä»å†å²å¯¹è¯è®°å½•`res['history']`ä¸­è·å–ã€‚

```python
import requests

url = 'http://xx.xx.xx.61:8777/api/local_doc_qa/local_doc_chat'
headers = {
    'content-type': 'application/json; charset=UTF-8'
}
data = {
    "user_id": "zzp",
    "kb_ids": ["KB527777624efb4ef097853c129bbfedc7_240625"], 
    "history": [],
    "question": "æˆ‘ä»Šå¹´57å²ï¼Œç´¯è®¡æŠ¤ç†é™©é£é™©ä¿é¢å·²ç»è¾¾åˆ°80ä¸‡ï¼Œæˆ‘è¿˜èƒ½å†è´­ä¹°xxxxé•¿æœŸæŠ¤ç†ä¿é™©ï¼ˆ3.0ç‰ˆï¼‰å—ï¼Ÿ",
    "source": "paas",
    "rerank": True,
    "custom_prompt": "ä½ æ˜¯xxå…¬å¸ï¼ˆä»¥ä¸‹ç®€ç§°xxï¼‰çš„ä¸€åèµ„æ·±å®¢æœä¸“å‘˜ï¼Œæ‹¥æœ‰ä¸°å¯Œçš„ä¿é™©çŸ¥è¯†å’ŒæœåŠ¡ç»éªŒï¼Œä¸»è¦ä¸ºä¸šåŠ¡å‘˜è§£ç­”æœ‰å…³ä¸ªäººäººèº«é™©æŠ•ä¿è§„åˆ™çš„é—®é¢˜ã€‚åœ¨æœåŠ¡è¿‡ç¨‹ä¸­è¯·éµå¾ªä»¥ä¸‹å‡ ç‚¹è¦æ±‚ï¼š1ã€ä¸å›ç­”çŸ¥è¯†åº“å†…å®¹ä¹‹å¤–çš„é—®é¢˜ï¼›2ã€è¯­è¨€é£æ ¼ç®€æ´ã€æ¸…æ™°ï¼›3ã€å¦‚æœé‡åˆ°ä¸ç¡®å®šçš„æƒ…å†µï¼Œå‘Šè¯‰ä¸šåŠ¡å‘˜ï¼š'æˆ‘ä¹Ÿä¸çŸ¥é“ï¼Œè¯·å’¨è¯¢äººå·¥å®¢æœï¼Œè°¢è°¢'",
    "api_base": "https://dashscope.aliyuncs.com/compatible-mode/v1",
    "api_key": "sk-xxxx",
    "model": "qwen-max",
    "max_token": 1024,
    "api_context_length": 16384,
    "chunk_size": 800,
    "top_p": 1,
    "temperature": 0.5
}

response = requests.post(url=url, headers=headers, json=data, timeout=600)
res = response.json()

# æå– source_documents ä¸­çš„ content ä½œä¸ºå¬å›æ–‡æœ¬å—
source_documents = res.get("source_documents", [])
contents = [doc.get("content", "") for doc in source_documents]

##################################
# åˆ›å»º SingleTurnSample å¯¹è±¡
from ragas import SingleTurnSample

sample = SingleTurnSample(
    # ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
    user_input=res['question'],
    # AI å¬å›çš„ç›¸å…³æ–‡æœ¬
    retrieved_contexts=contents,
    # AI ç»™å‡ºçš„ç­”æ¡ˆ
    response=res['history'][0][1],
    # äººç»™å‡ºçš„æ­£ç¡®å¬å›ç‰‡æ®µ
    # reference_contexts
    # äººç»™å‡ºçš„æ­£ç¡®ç­”æ¡ˆ
    reference="å½“è¢«ä¿äººå¹´é¾„ä¸º56è‡³60å‘¨å²æ—¶ï¼Œç´¯è®¡æŠ¤ç†é™©é£é™©ä¿é¢éœ€å°äºç­‰äº72ä¸‡å…ƒã€‚æ‚¨ä»Šå¹´57å²ï¼Œå·²ç»è¶…å‡ºç›¸åº”é¢åº¦é™åˆ¶ï¼Œå› æ­¤ä¸èƒ½å†è´­ä¹°é•¿æœŸæŠ¤ç†ä¿é™©ã€‚"
)

# ç”Ÿæˆè¯„ä¼°æ•°æ®é›†
from ragas import EvaluationDataset

eval_dataset = EvaluationDataset(samples=[sample])
```

### 3.2.2.å‰ç«¯ Bot å›ç­”

ç”±äºå‰ç«¯ Bot å·²ç»é…ç½®äº†å…³è”çŸ¥è¯†åº“å’Œæç¤ºè¯ï¼Œæ‰€ä»¥ä¸ä¸Šä¸€å°èŠ‚è°ƒç”¨ API ç›¸æ¯”ï¼Œå¯ä»¥å»æ‰`kb_ids`ï¼ˆçŸ¥è¯†åº“ idï¼‰å’Œ`custom_prompt`ï¼ˆæç¤ºè¯ï¼‰ï¼Œè€Œåªç”¨åŠ ä¸Š`bot_id`ã€‚è€Œå…·ä½“çš„`bot_id`å°±æ”¾åœ¨â€œæˆ‘çš„ botsâ€é¡µé¢çš„ URL ä¸­ï¼Œå¦‚<http://xx.xx.xx.61:8777/qanything/#/bots/BOT45d3b40f69ca4bae917a4e019a58a1cc/edit>ï¼Œé‚£ä¹ˆ`bot_id`å°±æ˜¯`BOT45d3b40f69ca4bae917a4e019a58a1cc`ã€‚

```python
import requests

url = 'http://xx.xx.xx.61:8777/api/local_doc_qa/local_doc_chat'
headers = {
    'content-type': 'application/json; charset=UTF-8'
}
data = {
    "user_id": "zzp",
    "bot_id":'BOT45d3b40f69ca4bae917a4e019a58a1cc',
    "history": [],
    "question": "æˆ‘ä»Šå¹´57å²ï¼Œç´¯è®¡æŠ¤ç†é™©é£é™©ä¿é¢å·²ç»è¾¾åˆ°80ä¸‡ï¼Œæˆ‘è¿˜èƒ½å†è´­ä¹°xxxxé•¿æœŸæŠ¤ç†ä¿é™©ï¼ˆ3.0ç‰ˆï¼‰å—ï¼Ÿ",
    "source": "saas_bot",
    "rerank": True,
    "api_base": "https://dashscope.aliyuncs.com/compatible-mode/v1",
    "api_key": "sk-xxxx",
    "model": "qwen-max",
    "max_token": 1024,
    "api_context_length": 16384,
    "chunk_size": 800,
    "top_p": 1,
    "temperature": 0.5
}

response = requests.post(url=url, headers=headers, json=data, timeout=600)
res = response.json()

# æå– source_documents ä¸­çš„ content ä½œä¸ºå¬å›æ–‡æœ¬å—
source_documents = res.get("source_documents", [])
contents = [doc.get("content", "") for doc in source_documents]

##################################
# åˆ›å»º SingleTurnSample å¯¹è±¡
from ragas import SingleTurnSample

sample = SingleTurnSample(
    # ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
    user_input=res['question'],
    # AI å¬å›çš„ç›¸å…³æ–‡æœ¬
    retrieved_contexts=contents,
    # AI ç»™å‡ºçš„ç­”æ¡ˆ
    response=res['history'][0][1],
    # äººç»™å‡ºçš„æ­£ç¡®å¬å›ç‰‡æ®µ
    # reference_contexts
    # äººç»™å‡ºçš„æ­£ç¡®ç­”æ¡ˆ
    reference="å½“è¢«ä¿äººå¹´é¾„ä¸º56è‡³60å‘¨å²æ—¶ï¼Œç´¯è®¡æŠ¤ç†é™©é£é™©ä¿é¢éœ€å°äºç­‰äº72ä¸‡å…ƒã€‚æ‚¨ä»Šå¹´57å²ï¼Œå·²ç»è¶…å‡ºç›¸åº”é¢åº¦é™åˆ¶ï¼Œå› æ­¤ä¸èƒ½å†è´­ä¹°é•¿æœŸæŠ¤ç†ä¿é™©ã€‚"
)

# ç”Ÿæˆè¯„ä¼°æ•°æ®é›†
from ragas import EvaluationDataset

eval_dataset = EvaluationDataset(samples=[sample])
```

## 3.3.è¯„ä¼° Dify

Dify ä¹Ÿæ˜¯åªèƒ½é€šè¿‡å‘æŒ‡å®š URL å‘é€è¯·æ±‚çš„æ–¹å¼æ¥è·å–ä¼šè¯æ•°æ®ã€‚

>æœ¬èŠ‚ä½¿ç”¨çš„ Dify ç‰ˆæœ¬æ˜¯0.15.1ã€‚

### 3.3.1.æ–°å»ºä¼šè¯

æ‰“å¼€ Difyï¼Œè¿›å…¥å·¥ä½œå®¤ï¼Œè¿›å…¥æ‰€é€‰æ‹©çš„åº”ç”¨ã€‚

+ é€‰æ‹©â€œç¼–æ’â€ï¼Œç‚¹å‡»é¡µé¢ç”±ä¸Šè§’çš„â€œåŠŸèƒ½â€ï¼Œæ‰“å¼€â€œå¼•ç”¨å’Œå½’å±â€çš„å¼€å…³ï¼Œè¿™æ ·æ‰ä¼šæ˜¾ç¤ºæºæ–‡æ¡£å’Œç”Ÿæˆå†…å®¹çš„å½’å±éƒ¨åˆ†ã€‚
+ é€‰æ‹©â€œè®¿é—® APIâ€ï¼Œè·å– API ç§˜é’¥ï¼Œæ ¹æ® API æ–‡æ¡£æŸ¥çœ‹å¦‚ä½•è°ƒç”¨å†å²ä¼šè¯è®°å½•
+ é€‰æ‹©â€œæ—¥å¿—ä¸æ ‡æ³¨â€å¯ä»¥æŸ¥çœ‹â€œç”¨æˆ·æˆ–è´¦æˆ·â€å’Œâ€œä¼šè¯idâ€ä¿¡æ¯ï¼Œå¯ä»¥è·å–å›ºå®šä¼šè¯ id çš„ä¿¡æ¯ã€‚

```python
import requests
qa_url = 'http://xx.xx.xx.69/v1/chat-messages'
qa_headers = {
    'content-type': 'application/json; charset=UTF-8',
    'Authorization': 'Bearer app-nSMee7Zeg1Km8z8l8NdGue5P'
}

question = "æˆ‘ä»Šå¹´57å²ï¼Œç´¯è®¡æŠ¤ç†é™©é£é™©ä¿é¢å·²ç»è¾¾åˆ°80ä¸‡ï¼Œæˆ‘è¿˜èƒ½å†è´­ä¹°xxxxé•¿æœŸæŠ¤ç†ä¿é™©ï¼ˆ3.0ç‰ˆï¼‰å—ï¼Ÿ"
qa_input = {
    "inputs": {},
    "query": question,
    "response_mode": "blocking",
    "conversation_id": "",
    "user": 'é˜¿æœ¨ç‹—',
    "files": {}
}

response = requests.post(url=qa_url, headers=qa_headers, json=qa_input, timeout=600)
res = response.json()

# æå– metadata é‡Œé¢ retriever_resources ä¸­çš„ content ä½œä¸ºå¬å›æ–‡æœ¬å—
retriever_resources = res.get('metadata', {}).get('retriever_resources', []) 
contents = [doc.get("content", "") for doc in retriever_resources]

##################################
# åˆ›å»º SingleTurnSample å¯¹è±¡
from ragas import SingleTurnSample

sample = SingleTurnSample(
    # ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
    user_input=question,
    # AI å¬å›çš„ç›¸å…³æ–‡æœ¬
    retrieved_contexts=contents,
    # AI ç»™å‡ºçš„ç­”æ¡ˆ
    response=res['answer'],
    # äººç»™å‡ºçš„æ­£ç¡®å¬å›ç‰‡æ®µ
    # reference_contexts
    # äººç»™å‡ºçš„æ­£ç¡®ç­”æ¡ˆ
    reference="å½“è¢«ä¿äººå¹´é¾„ä¸º56è‡³60å‘¨å²æ—¶ï¼Œç´¯è®¡æŠ¤ç†é™©é£é™©ä¿é¢éœ€å°äºç­‰äº72ä¸‡å…ƒã€‚æ‚¨ä»Šå¹´57å²ï¼Œå·²ç»è¶…å‡ºç›¸åº”é¢åº¦é™åˆ¶ï¼Œå› æ­¤ä¸èƒ½å†è´­ä¹°é•¿æœŸæŠ¤ç†ä¿é™©ã€‚"
)

# ç”Ÿæˆè¯„ä¼°æ•°æ®é›†
from ragas import EvaluationDataset

eval_dataset = EvaluationDataset(samples=[sample])
```

### 2.5.2.å†å²ä¼šè¯

ä¸‹é¢è¿™æ ·å¾—åˆ°çš„å†å²æ•°æ®åªæœ‰é—®é¢˜ï¼Œæ²¡æœ‰ç­”æ¡ˆå’Œå¬å›æ–‡æœ¬ç‰‡æ®µï¼Œè§£å†³åŠæ³•è¿˜æœ‰å¾…æ¢ç´¢ã€‚

```python
import requests

url = 'http://xx.xx.xx.69/v1/conversations'
params = {
    'user': 'abc-123',
    'conversation_id': '4d1d8c13-84b1-4911-b408-510880397f5c',
    'last_id': '',
    'limit': 20,
}
headers = {
    'Authorization': 'Bearer app-nSMee7Zeg1Km8z8l8NdGue5P'
}

response = requests.get(url, params=params, headers=headers)
res = response.json()
```
