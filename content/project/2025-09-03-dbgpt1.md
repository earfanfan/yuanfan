---
title: 初试 DB-GPT
author: yuanfan
date: 2025-09-07T19:58:55+0800
slug: DBGPT1
categories:
  - AI
tags:
  - AI
draft: no
---

<!--more-->

今年我的工作被派了一个项目任务，名为“ AI 的研究与落地”。到了2025年才开始琢磨这项工作，显然不会是最早的那批人，但好处是各种 AI 相关的开源项目都已经迭代很多版本，发展相对成熟，所以我主要去做的事情偏向于研究如何用好那些工具。而具体的内容也有过几次转变，从 知识问答（RAG）到问数再到辅助分析，琢磨知识问答的时候理解了分片、召回，琢磨问数的时候理解了流程设计，现在琢磨辅助分析，之前所学全都抛弃，只剩下应用场景。

本文算是学习使用 DB-GPT 这个工具的笔记，照例从部署安装开始记录。

>截止记录笔记时，下载的 DB-GPT 最新版本是0.7.3。

# 一、部署

## 1.1.服务器联网部署

这里说明几个前置条件：1.在 Linux 服务器部署；2.挂了代理可以连接外网；3.Docker 可用。 

在可以联网的情况下，部署的方法很简单，即从外网获取官方提供的仓库放到服务器，然后启动就行。获取官方仓库的方式有两种：其一，从 DockerHub 拉取官方镜像仓库；其二，从 github 克隆仓库源码。执行代码启动镜像后，在浏览器打开<服务器 IP:5670>就能访问页面。

如果本来就注册了硅基流动（siliconflow）的 API KEY，那么第一种方法直接可用。否则的话，需要参考下一小节的内容稍作修改，才能用第二种方法。

```bash
# 检查 docker 状态是否正常
sudo systemctl status docker

# 1. 从 dockerhub 拉取官方镜像仓库
docker pull eosphorosai/dbgpt:latest

# 启动镜像
docker run -it --rm -e SILICONFLOW_API_KEY=${sk-xxxxxx} \
 -p 5670:5670 --name dbgpt eosphorosai/dbgpt-openai
# 关闭按组合键 ctrl+ C 

# 2. 从 github 克隆仓库源码，指定目录
git clone https://github.com/eosphoros-ai/DB-GPT.git /user/hadoop/DBGPT/  

cd /user/hadoop/DBGPT/
# 启动
docker compose up -d
# 关闭
docker compose down
```

## 1.2.改用通义千问的 API KEY

若是进入项目文件所在目录，使用`docker compose`来启动镜像，启不起来的话就需要去翻查对应的`docker-compose.yml`文件，查看各个配置项的细节。在刚下载下来的配置文件里，下面这一部分默认使用硅基流动（siliconflow），我的需求是改用通义千问，于是修改两处，一是改成引用通义的模型配置文件（`.toml`），二是改为调用通义的 API KEY。

+ 修改前

```yaml
webserver:
    image: eosphorosai/dbgpt-openai:latest
    command: dbgpt start webserver --config /app/configs/dbgpt-proxy-siliconflow-mysql.toml
    environment:
      - SILICONFLOW_API_KEY=${SILICONFLOW_API_KEY}
```

+ 修改后

```yaml
webserver:
    image: eosphorosai/dbgpt-openai:latest
    command: dbgpt start webserver --config /app/configs/dbgpt-proxy-tongyi.toml
    environment:
      - DASHSCOPE_API_KEY=sk-xxxxxx
```

这个版本的 DB-GPT 在切换模型来源和切换模型这两件事上不如 DIFY 简单。如果要切换模型来源，需要手动修改`docker-compose.yml`文件，然后重启；如果要切换模型，也需要修改对应的模型配置文件，比如往 `dbgpt-proxy-tongyi.toml`里面增加调用更多模型的配置。

这里踩了一个坑，`dbgpt-proxy-tongyi.toml`文件里面原本只配置了 llm 和 embedding 两种模型，而我想当然地给加上 rerank 模型的配置信息，重启后发现出了问题。执行`docker ps`发现相关镜像的状态一直是`restart`，这代表启动失败，再执行`docker logs <容器ID或名称>`查看报错原因，错误如下。后来把配置 rerank 模型的信息删掉，然后再重启就正常了。

>ValueError: Invalid data for ModelsDeployParameters: Invalid list element type: Invalid data for RerankerDeployModelParameters: Unknown type value: proxy/tongyi, known types: ['hf', 'qwen', 'proxy/openapi', 'proxy/siliconflow', 'proxy/tei', 'proxy/infiniai']

## 1.3.部到另一套服务器上

由于一些复杂的权限隔离、网络安全的问题，第一次部署的服务器挂了代理能连上外网，但是使用起来很慢，这里要再部到另一台服务器上，虽然不能连外网但是专门给通义千问的 API KEY 开通了策略，使用起来会很快。

我是从 github 克隆仓库源码，整个项目目录大概如下。

+ `docker-compose.yml`文件
+ docker 文件夹
+ configs 文件夹
  - `dbgpt-proxy-tongyi.toml`文件
+ 其他文件或文件夹  

照理说只要打包镜像文件和 docker 文件夹，放到另一台服务器也能启用，但在踩坑折腾的过程中，我比较习惯于打开文件目录根据报错信息去翻项目文件（ps 打开 Xftp 查看），要是只打包 docker 文件夹，再翻项目文件得一层层进镜像里面查找，所以最后选择整个项目文件全打包过去。

```bash
# 原服务器：打包镜像文件
docker save -o /user/hadoop/DBGPT_images_20250903.tar eosphorosai/dbgpt-openai:latest mysql/mysql-server

# 原服务器：压缩整个项目文件
cd /user/hadoop
tar -czvf DBGPT.tar.gz docker

# 两个文件转移到新服务器 /user/model 目录下

# 新服务器：解压缩项目文件
cd /user/model/
tar -xzvf DBGPT.tar.gz -C /user/model/

# 新服务起：加载镜像文件
docker load -i /user/model/DBGPT_images_20250903.tar

# 启动
cd /user/model/DBGPT
docker compose up -d
```

## 1.4.配置测试数据库

在官方文档<http://docs.dbgpt.cn/docs/application/apps/chat_dashboard>里面，配置官方准备好的测试数据库很简单，不过对于不熟悉项目的人比如本菜鸟，看着会很懵。

>In order to better experience the report analysis capabilities, we have built some test data into the code. To use this test data, we first need to create a test library.

>>`CREATE DATABASE IF NOT EXISTS dbgpt_test CHARACTER SET utf8;`

>After the test library is created, you can initialize the test data with one click through the script.

>>`python docker/examples/dashboard/test_case_mysql_data.py`

```bash
# 查看容器 id
docker ps 
# 进 mysql 容器(dbgpt-db-1)
docker exec -it 6eee67836906 bash
# 进 mysql，输入密码 aa123456
mysql -u root -p
# 建数据库
CREATE DATABASE IF NOT EXISTS dbgpt_test CHARACTER SET utf8;
# 查看是否建成功
show database;
# 退出 mysql，退出容器
exit
exit

# 再进 dbgpt 容器(dbgpt-webserver-1)
docker exec -it c0b37ad462e1 bash
# 执行 .py 脚本 写入数据 
python /app/docker/examples/dashboard/test_case_mysql_data.py
```

报错连不上 mysql。

>pymysql.err.OperationalError: (2003, "Can't connect to MySQL server on '127.0.0.1' ([Errno 111] Connection refused)")

翻查被执行的`test_case_mysql_data.py`，发现关于连接的配置代码如下。参照项目主目录的`docker-compose.yml`文件，有两个问题：一是默认值"127.0.0.1"在这里不好用，会导致导致连不上 mysql，而 mysql 对应的服务名应该是“db”；二是`test_case_mysql_data.py`文件中的 mysql 登录密码是 aa12345678，但是项目主目录的`docker-compose.yml`文件中默认的 mysql 登录密码是 aa123456。

```bash
if __name__ == "__main__":
    connection = pymysql.connect(
        host=os.getenv("DB_HOST", "127.0.0.1"),
        port=int(
            os.getenv("DB_PORT", 3306),
        ),
        user=os.getenv("DB_USER", "root"),
        password=os.getenv("DB_PASSWORD", "aa12345678"),
        database=os.getenv("DB_DATABASE", "dbgpt_test"),
        charset="utf8mb4",
        ssl_ca=None,
    )
```

不知何故，以上两处差异修改后仍然报错。接下来，直接在 dbgpt 容器中执行下列代码，从执行后的日志可以看到数据已经被正常写入。

```bash
DB_HOST=db \
DB_PORT=3306 \
DB_USER=root \
DB_PASSWORD=aa123456 \
DB_DATABASE=dbgpt_test \
python /app/docker/examples/dashboard/test_case_mysql_data.py
```

下面再检测一遍从 dbgpt 容器是否能正常连接 mysql，依然是在 dbgpt 容器中，使用`cat`创建多行脚本，执行`cat > test_mysql.py`，接着复制粘贴以下代码，随后按组合键 CTRL + D 保存并退出。最后执行`python test_mysql.py`，可以看到连接成功。

```python
import pymysql

try:
    conn = pymysql.connect(
        host="db",
        user="root",
        password="aa123456",
        database="dbgpt_test",
        port=3306
    )
    print("✅ MySQL 连接成功！")
except Exception as e:
    print("❌ 连接失败：", e)
```

以上步骤分别是在 mysql 创建数据库，通过 python 脚本添加数据，测试能否正常连接。

接下来还要在浏览器中打开页面，在数据库菜单再手动配置一下，之后才能在应用中正常使用测试数据库。根据官方文档的演示截图中，依次输入下面三段文字，输出的结果和截图中并不一样。

>help me build a sales report summarizing our key metrics and trends.

>Please create an analysis reposrt to help improve product operation efficiency and increase overall sales.

>analyze user orders from multiple dimensions.

# 二、使用

DB-GPT 提供了六大项基础应用，本文暂不涉及 Chat Normal 和 Chat Knowledge。 

|应用名称|数据来源|生成与执行 SQL|输出形式|输出内容|典型用途|
|:--:|:--:|:--:|:--:|:--:|:--:|
|Chat Dashboard|配置数据库|多段 SQL（执行）|仪表板|Preview 页面展示可视化报告、Editor 页面展示 SQL|生成报告|
|Chat Data|配置数据库|一段 SQL（执行）|问答页面|分析思路、Chart（图形）、SQL、Data（数据）|辅助数据分析|
|Chat Excel|上传 csv 格式文件|一段 SQL（执行）|问答页面|分析思路、Chart（图形）、SQL、Data（数据）|辅助数据分析|
|Chat DB|配置数据库|一段 SQL（不执行）|问答页面|SQL及字段释义|了解数据的基础信息|

以上四种应用，可用于以下场景。

1. 懂业务、不懂 SQL，了解数据，使用 Chat Excel 辅助数据分析与洞察。
2. 懂业务、懂 SQL，不了解数据库中表信息的，先使用 Chat DB 了解数据，然后使用 Chat Data 辅助数据分析与洞察。
3. 懂业务、懂 SQL、了解数据，使用 Chat Dashboard 生成报告。

下面是最近试用过程中踩的几个坑，因为使用时间较少，许多地方也还没摸透。

## 2.1.Chat Excel 

> EXCEL 文件需要转换为`.csv`格式！

我试了一下，如果上传.xlsx 文件（大小约 15K ），半小时后也能传成功，翻查日志找到以下警告，猜测效率慢的原因有可能是当前版本没有集成解析 EXCEL 格式的插件。后来转成 .csv 格式，30秒能传完。

>dbgpt_app.scene.chat_data.chat_excel.excel_reader[1] WARNING Error while reading file: An error occurred while trying to automatically install the required extension 'excel': Failed to download extension "excel" at URL "http://extensions.duckdb.org/v1.2.2/linux_amd64_gcc4/excel.duckdb_extension.gz"
Extension "excel" is an existing extension.For more info, visit https://duckdb.org/docs/extensions/troubleshooting/?version=v1.2.2&platform=linux_amd64_gcc4&extension=excel (ERROR Could not establish connection)

用户输入后，此应用一般会输出4个部分的内容：分析思路、Chart（图形）、SQL、Data（数据）。如果分析思路中不包含绘图，那么仅展示 SQL 和 Data。

## 2.2.Chart Data

用户输入后，报错，错误是“Generate view content failed”。

翻查镜像日志，查看到错误如下。这是因为数据库的表中有个字段名称是`YEAR_MONTH`，这与 MYSQL 有关键词冲突，如果执行的 SQL 语句里面 不对该字段增加反引号，会导致在 MYSQL 中执行时报错。

>2025-09-05 08:38:58 a41259b731e6 dbgpt.datasource.rdbms.base[1] ERROR Error in session scope: (pymysql.err.ProgrammingError) (1064, "You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'YEAR_MONTH, (add_agents - loss_agent) AS active_agents, income FROM branch_data1' at line 1")

解决方法是，要么改字段名称，要么调用其他 AI 。

## 2.3.Chart Dashboard

下面仅仅只是瞎碰乱试得到的一些结论，偏于暂时性质的。

1. 使用时需要配置合适的知识库和提示词。

比如我在造数测试的时候造了“百元标保成本”和“规模保费收入”，然后输入内容提到让 AI 去分析利润或收益，输出的内容会将这两个指标直接相减，事实上在保险行业这两个指标根本不是一个数量级，不能相减。我猜的是，在垂直领域，需要给 AI 外挂好用的知识库，不然它真会缺少行业常识。

2. 规划数据时，维度要保持统一

起初，在我配置直连的数据库里，有的表里分公司的取值是河北、湖北等真实信息，有的表里是编造的分公司A、分公司B等虚假信息，由于 AI 读入暂时只有表结构信息，没有整张表的数据，导致它输出的多个图形里两种信息都存在。

3. 尽量让各个表中指标不要重复。

如果编写的指标名称、释义（comments）出现重复，AI 会瞎关联。

# 三、一些思考

1. DB-GPT 只展示数据，不给出结论。

DB-GPT的分析受限于图形的展现形式，而如何解读图形和数据又是另一件事。从我脑子里统计学的视角来看，只有足够多的数据才能让 AI 辅助分析后做出最贴合现实的决策，但事实是，现在 AI 的能力还不足以获取人类百分百的信任，AI 进入企业后还需要接受权限管控。

2. 精确的需求，具体的分析。

企业里通常存在这样一个场景，领导突然关注某个新问题，属下员工去搜集数据然后分析，最后汇报分析结果。如果 AI 不能完全取信于人类，也要面临权限隔离，那么那个做数据分析的员工无法被 AI 取代，因为领导总是只能描述出偏于宏观的、模糊的需求，这会导致 AI 无法给领导一份满意的报告。目前来看，DB-GPT 可能在辅助分析方面提高人类的工作效率，但无法将人类员工全都取代。

3. 对比辅助编程与辅助数据分析

最近和同事讨论 AI 辅助编程和 AI 辅助数据分析哪个能走更远。

第一位同事的观点是，将来应该赋予 AI 自动编译代码的权限，给了AI 执行命令的权限，AI 才能发现错误并自己解决，AI 辅助编程一直是朝着一体化来发展的，一句话就能把需求分析、开发、测试、问题修改、发布都搞定。将来 AI 可以永生，AI 提供基础服务，人类做自己想做的事就好了

第二位同事的观点是，这有点像是 AI 的共产主义，但现在 AI 还没诞生自主意识，并且代码质量能否过关仍未可知（ps我们公司最近引入 AI 辅助编程工具，但对于 IT 工作效率能提高多少、缺陷率能降低多少仍然需要观望）。

我的观点是，按照第一位同事所说，这样的 AI 接近于永动机模式，另外 AI 可以执行具体的指令，可是人类总是表达模糊的需求。在当前社会，人类融入社会分工中，被家庭、工作、社会关系等赋予活着的意义（ps也不一定是意义，或者可以说是人类意识形态里认为活着想做的事），要是这些东西一口气全都解体，人类会陷入迷茫。

官方文档中有几篇值得多翻翻，或许对于将来给领导讲故事有用，下面把链接放这里。

参考文档：
1. DB-GPT 的架构 <https://www.yuque.com/eosphoros/dbgpt-docs/eg8lvydr5rvzmhpc>
2. 基于 DB-GPT 的财报分析助手 <https://www.yuque.com/eosphoros/dbgpt-docs/cmogrzbtmqf057oe>
3. GPT-Vis 知识库总览<https://github.com/antvis/GPT-Vis/blob/main/knowledges/%E7%9F%A5%E8%AF%86%E5%BA%93%E6%80%BB%E8%A7%88.md>
